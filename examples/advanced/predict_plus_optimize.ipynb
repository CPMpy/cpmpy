{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cpmpy import *\n",
    "## torch for training neural network\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.utils.data as data_utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "## sklean module for data transforming and model running\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "import sklearn\n",
    "#seaborn for creating nice plot\n",
    "import seaborn as sns\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "# sns.set_theme()\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "import os\n",
    "from sklearn.model_selection import learning_curve\n",
    "import shutil\n",
    "if os.path.exists('runs/CPMPY'):\n",
    "    shutil.rmtree('runs/CPMPY')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the following cell is kept with initial cell, it does not remove the directory!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "## tensorboard to look at learning curve\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('runs/CPMPY')\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we write a knapsack solver using CPMpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class problem_solver:\n",
    "    def __init__(self,weights,capacity,n_items):\n",
    "        self.weights = weights\n",
    "        self.capacity =  capacity\n",
    "        self.n_items = n_items\n",
    "        self.x = boolvar(shape=n_items, name=\"x\")\n",
    "        self.model = Model(sum(self.x*weights[0]) <= capacity)\n",
    "        \n",
    "    def solve_knapsack(self,cost):\n",
    "        cost_ = (1000*cost).astype(int)\n",
    "        # We have to convert the cost variable into integer, as ORtools only accepts integer variable\n",
    "        self.model.maximize((self.x*cost_).sum())\n",
    "        self.model.solve()\n",
    "        return self.x.value()\n",
    "        \n",
    "        \n",
    "    def solve_from_pytorch(self,cost_inpytorch):\n",
    "        # cost_inpytorch is a pytorch tensor object\n",
    "        cost_ = cost_inpytorch.detach().numpy()\n",
    "        x = self.solve_knapsack(cost_)\n",
    "        return torch.from_numpy(x).float()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us use it to solve a simple knapsack problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knapsack_solver = problem_solver(weights = [[1,2,3,1]], capacity=3,n_items=4)\n",
    "knapsack_solver.solve_knapsack(np.array([10,20,25,15]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this example, out of the 4 objects, the 2nd and the 4th one are selcted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will use this module to implement the **SPO approach**  of `Elmachtoub, Adam N., and Paul Grigas. \"Smart “predict, then optimize”.\" Management Science (2021).`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we  load the data. We have training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26496, 8)\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('Data.npz'):\n",
    "    torch.hub.download_url_to_file(\"https://github.com/JayMan91/aaai_predit_then_optimize/raw/master/Data.npz\", \"Data.npz\")\n",
    "data = np.load(\"Data.npz\")\n",
    "x_train,  x_test, y_train,y_test = data['X_1gtrain'],data['X_1gtest'],data['y_train'],data['y_test']\n",
    "x_train = x_train[:,1:]\n",
    "x_test = x_test[:,1:]\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8 features: HolidayFlag, DayOfWeek, WeekOfYear, Month, ForecastWindProduction, Forecasted SystemLoad,  Forecasted Energy price, Forecasted CO2Intensity\n",
    "#### And the predictor variable: Energy Price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we standardize the feature variables. We also reshape the data in groups of 48 so that each group is an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in training (552, 48, 8) and testing (50, 48, 8)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_train = x_train.reshape(-1,48,x_train.shape[1])\n",
    "y_train = y_train.reshape(-1,48)\n",
    "x_test = x_test.reshape(-1,48,x_test.shape[1])\n",
    "y_test = y_test.reshape(-1,48)\n",
    "x_test = x_test[:50]\n",
    "y_test = y_test[:50]\n",
    "print(\"Number of instances in training {} and testing {}\".format(x_train.shape, x_test.shape))\n",
    "## we now randomize the training and test data\n",
    "x = np.concatenate((x_train, x_test), axis=0)\n",
    "y = np.concatenate((y_train,y_test), axis=0)\n",
    "x,y = sklearn.utils.shuffle(x,y,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we see, we have 552 training instances and test on 50 instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use this data in the context of knapsack problem. The predicted  Energy Price forms the value of knapsack. In our setup, each knapsack instance consists of 48 items. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n"
     ]
    }
   ],
   "source": [
    "weights = [data['weights'].tolist()]\n",
    "weights = np.array(weights)\n",
    "n_items = 48\n",
    "capacity = 60\n",
    "print(np.sum(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The weights are the costs of each item. The weights are kept same for each instance. The summation of 48 weights are 240. The knapsack capacity in this example is 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we create a solver instance with the specified weights and capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "knapsack_solver = problem_solver(weights,capacity,n_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The desirable goal to minimze the regret,\n",
    "### Regret is the loss in the objective value for using the solution of predicted value rather than the solution of true value (which we do not know).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If $c$ are the true values and $\\hat{c}$ are the predicted values and $w^\\star(c)$ and $w^\\star(\\hat{c})$ are their solutions. \n",
    "### Then regret $=c^\\top( w^\\star(c) -w^\\star(\\hat{c}) )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A function to compute regret\n",
    "def regret_score(y,y_pred):\n",
    "        total_loss =0 \n",
    "        for i in range(len(y)):\n",
    "            sol_true = knapsack_solver.solve_knapsack(y[i])\n",
    "            sol = knapsack_solver.solve_knapsack(y_pred[i])\n",
    "            total_loss +=  ((sol_true - sol).dot(y[i]))\n",
    "        return total_loss/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use the sklearn module for model training and scoring. For that we need a scoring function which will compute average regret of the instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def score_fucn(model,X,y):\n",
    "    model.eval()\n",
    "    y_pred = model(torch.from_numpy(X).float()).squeeze()\n",
    "    regret = regret_score(y,y_pred)\n",
    "    model.train()\n",
    "    return regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will also need a dataloder module, which will segregate the training data into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader():\n",
    "    def __init__(self, X,y):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "\n",
    "        \n",
    "        sol = []\n",
    "        for i in range(len(y)):\n",
    "            x_sol = knapsack_solver.solve_knapsack(y[i])            \n",
    "            sol.append( x_sol)\n",
    "        self.sol = np.array(sol)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return  self.X[idx],self.y[idx],self.sol[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-stage Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class reg:\n",
    "    def __init__(self, net = nn.Linear(8,1),\n",
    "                 epochs=8,optim=optim.Adam,batch_size=24,lr=1,**kwg):\n",
    "        self.net = net\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.kwg = kwg\n",
    "        self.optim = optim\n",
    "        self.lr = lr\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        para_dict =  {\"net\": self.net, \"epochs\": self.epochs,\n",
    "        \"batch_size\":self.batch_size,\n",
    "        \"optim\":self.optim,\"lr\":self.lr}\n",
    "        return para_dict\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        self.model = self.net\n",
    "        self.optimizer = self.optim(self.model.parameters(),lr=self.lr, **self.kwg)\n",
    "        criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "        train_df = Dataloader(X,y)\n",
    "        fit_time= 0\n",
    "        \n",
    "        for e in tqdm(range(self.epochs)):\n",
    "            train_dl = data_utils.DataLoader(train_df, batch_size=self.batch_size,shuffle=True)\n",
    "            bb = 0\n",
    "            for x_train,y_train,sol_train in train_dl:\n",
    "                start_time = time.time()\n",
    "                self.optimizer.zero_grad()\n",
    "                y_pred = self.model(x_train).squeeze()\n",
    "                loss= criterion(y_pred,y_train)\n",
    "                loss.retain_grad()\n",
    "                y_pred.retain_grad()\n",
    "                loss.backward()                \n",
    "                self.optimizer.step()\n",
    "                end_training = time.time()\n",
    "                ## now  we will save mse and regret and traininng data, this will take more time\n",
    "                if (e)%3==0:\n",
    "                    mse_loss =  loss.item()\n",
    "                    regret_loss = regret_score(y_train.detach().numpy(),y_pred.detach().numpy())\n",
    "                    end_regret = time.time()\n",
    "                    fit_time += end_training - start_time\n",
    "                    writer.add_scalar('Two stage MSE loss',mse_loss, e + bb/self.batch_size, fit_time)\n",
    "                    writer.add_scalar('Two stage Regret',regret_loss, e + bb/self.batch_size, fit_time)\n",
    "                bb =+1\n",
    "\n",
    "    def score(self,X,y):\n",
    "        return score_fucn(self.model,X,y)\n",
    "\n",
    "    def predict(self,X):\n",
    "        self.model.eval()\n",
    "        y_pred = self.model(torch.from_numpy(X).float()).squeeze()\n",
    "        self.model.train()\n",
    "        return y_pred.detach().numpy()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A wrapper module to to train and test multiple times. We will use the inbuilt cross_validate module to train and test `n_run` times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeatrun(n_train, n_test,n_run=1):\n",
    "    i = 0\n",
    "    while i < n_run:\n",
    "        idx1 = np.arange(0, n_train , dtype=int)\n",
    "        idx2 = np.arange(n_train, n_train+n_test, dtype=int)\n",
    "        yield idx1, idx2\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will run the two-stage model and save the regret and MSE  on test and train data\n",
    "#### We also monitor the leanring curve. It will take 1 minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:48<00:00,  4.83s/it]\n"
     ]
    }
   ],
   "source": [
    "regression_df = pd.DataFrame()\n",
    "epoch=10\n",
    "scoring = {'Regret': make_scorer(regret_score),\n",
    "            'MSE': make_scorer(mse)}\n",
    "model = reg(optim=optim.SGD, epochs=epoch,lr= 0.01)\n",
    "cv = repeatrun(len(y_train),len(y_test))\n",
    "scores = cross_validate(model, x,y, scoring=scoring,cv=cv, return_train_score=True)\n",
    "scores = pd.DataFrame.from_dict(scores)\n",
    "scores['epoch'] = epoch\n",
    "regression_df = regression_df.append(scores)\n",
    "regression_df['model'] =\"Two-stage\"\n",
    "regression_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will examine the learning curve on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6005 (pid 6943), started 2:21:42 ago. (Use '!kill 6943' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1e965fd5ffd5ede\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1e965fd5ffd5ede\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6005;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=runs --port=6005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smart Predict and Optimize Regression Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPO:\n",
    "    def __init__(self, net = nn.Linear(8,1),\n",
    "                 epochs=8,optim=optim.Adam,batch_size=24,lr=1,\n",
    "                 **kwg):\n",
    "        self.net = net\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.kwg = kwg\n",
    "        self.optim = optim\n",
    "        self.lr = lr\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        para_dict =  {\"net\": self.net, \"epochs\": self.epochs,\n",
    "        \"batch_size\":self.batch_size,\"optim\":self.optim,\n",
    "        \"lr\":self.lr}\n",
    "        return para_dict\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        self.model = self.net\n",
    "        self.optimizer = self.optim(self.model.parameters(),lr=self.lr, **self.kwg)\n",
    "        \n",
    "        train_df = Dataloader(X,y)\n",
    "        criterion = nn.MSELoss(reduction='mean')\n",
    "        fit_time= 0\n",
    "        \n",
    "        for e in tqdm(range(self.epochs)):\n",
    "            train_dl = data_utils.DataLoader(train_df, batch_size=self.batch_size,shuffle=True)\n",
    "            bb = 0\n",
    "            for x_train,y_train,sol_train in train_dl:\n",
    "                start_time = time.time()\n",
    "                self.optimizer.zero_grad()\n",
    "                y_pred = self.model(x_train).squeeze()\n",
    "                \n",
    "                # The difference from the Two-stage approah is that,\n",
    "                # the loss function considers what happen after solving the knapsack instances\n",
    "                y_spo = 2*y_pred - y_train\n",
    "                grad_list = []\n",
    "                # we have to solve the knpsack problem for all training instances with the predicted energy price\n",
    "                for i in range(len(y_train)):\n",
    "                    sol_spo = knapsack_solver.solve_from_pytorch(y_spo[i])\n",
    "                    # We compute the (sub)gradients directly, see SPO paper\n",
    "                    grad = sol_spo - sol_train[i]\n",
    "                    grad_list.append(grad)\n",
    "                grad = torch.stack(grad_list,0)\n",
    "                y_pred.retain_grad()\n",
    "                # gradients do not come from a loss , but we use SPO subgradients   \n",
    "                y_pred.backward(gradient=grad)\n",
    "                # This is where SPO differes from the Two-stage model\n",
    "                self.optimizer.step()\n",
    "                end_training = time.time()\n",
    "                ## now  we will save mse and regret and traininng data, this will take more time\n",
    "                if (e)%3==0:\n",
    "                    mse_loss = criterion(y_pred,y_train).item()\n",
    "                    regret_loss = regret_score(y_train.detach().numpy(),y_pred.detach().numpy())\n",
    "                    end_regret = time.time()\n",
    "                    fit_time += end_training - start_time\n",
    "                    writer.add_scalar('SPO MSE loss',mse_loss, e + bb/self.batch_size,fit_time)\n",
    "                    writer.add_scalar('SPO Regret',regret_loss, e + bb/self.batch_size,fit_time)\n",
    "                bb =+1\n",
    "\n",
    "    def score(self,X,y):\n",
    "        return score_fucn(self.model,X,y)\n",
    "\n",
    "    def predict(self,X):\n",
    "        self.model.eval()\n",
    "        y_pred = self.model(torch.from_numpy(X).float()).squeeze()\n",
    "        self.model.train()\n",
    "        return y_pred.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will run the SPO model and save the regret and MSE  on test and train data\n",
    "####  We also monitor the leanring curve. It will take 2-3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:48<00:00, 10.87s/it]\n"
     ]
    }
   ],
   "source": [
    "spo_df = pd.DataFrame()\n",
    "\n",
    "scoring = {'Regret': make_scorer(regret_score),\n",
    "            'MSE': make_scorer(mse)}\n",
    "model = SPO(optim=optim.Adam, epochs=epoch,lr= 0.01)\n",
    "cv = repeatrun(len(y_train),len(y_test))\n",
    "scores = cross_validate(model, x,y, scoring=scoring,cv=cv, return_train_score=True)\n",
    "scores = pd.DataFrame.from_dict(scores)\n",
    "scores['epoch'] = epoch\n",
    "spo_df = spo_df.append(scores)\n",
    "spo_df['model'] =\"SPO\"\n",
    "spo_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will see the learning curve on tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6005 (pid 6943), started 2:21:18 ago. (Use '!kill 6943' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f8d179bf0b06f7ec\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f8d179bf0b06f7ec\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6005;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=runs --port=6005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For a two-stage model even though MSE goes down, regret goes up\n",
    "## For SPO, Regret goes down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we examine the performance on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>test_Regret</th>\n",
       "      <th>test_MSE</th>\n",
       "      <th>fit_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SPO</th>\n",
       "      <td>767.168383</td>\n",
       "      <td>143055.896013</td>\n",
       "      <td>115.545454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Two-stage</th>\n",
       "      <td>972.689750</td>\n",
       "      <td>32216.110256</td>\n",
       "      <td>54.508035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          test_Regret       test_MSE    fit_time\n",
       "                 mean           mean        mean\n",
       "model                                           \n",
       "SPO        767.168383  143055.896013  115.545454\n",
       "Two-stage  972.689750   32216.110256   54.508035"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rslt_df =  pd.concat([spo_df,regression_df],ignore_index=True)\n",
    "rslt_df.groupby(['model']).agg({'test_Regret':['mean'],'test_MSE': ['mean'],'fit_time':['mean'] })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We see, on test data, regret is significantly lower for SPO. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We demonstrate for an prediction+optimisation problem, SPO generates predictions that are better suited to the optimisation problem, that is, a lower regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Because we repeatedly call solver while training SPO model, the model fitting time is high for SPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Implementation with growth #####\n",
    "class problem_solver:\n",
    "    def __init__(self,weights,capacity,n_items, grow=0.01):\n",
    "        self.weights = weights\n",
    "        self.capacity =  capacity\n",
    "        self.n_items = n_items\n",
    "        self.x = boolvar(shape=n_items, name=\"x\")\n",
    "        self.pool = np.empty([0, n_items])\n",
    "        self.grow = grow\n",
    "        self.model = Model(sum(self.x*weights[0]) <= capacity)\n",
    "        \n",
    "    def solve_knapsack(self,cost):\n",
    "        cost_ = (1000*cost).astype(int)\n",
    "        # We have to convert the cost variable into integer, as ORtools only accepts integer variable\n",
    "        self.model.maximize((self.x*cost_).sum())\n",
    "        self.model.solve()\n",
    "        return self.x.value()\n",
    "        \n",
    "        \n",
    "    def solve_from_pytorch(self,cost_inpytorch):\n",
    "        # cost_inpytorch is a pytorch tensor object\n",
    "        cost_ = cost_inpytorch.detach().numpy()\n",
    "        if (np.random.random(1)[0]< self.grow) or len(self.pool)==0:\n",
    "            x = self.solve_knapsack(cost_)\n",
    "            self.pool = np.unique(np.append(self.pool,np.array([x]),axis=0),axis=0)\n",
    "        else:\n",
    "            x = self.pool[np.argmax((self.pool*cost_).sum(1))]\n",
    "        return torch.from_numpy(x).float()\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
